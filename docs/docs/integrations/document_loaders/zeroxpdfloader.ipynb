{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# ZeroxPDFLoader\n",
    "\n",
    "This notebook provides a quick overview for getting started with `ZeroxPDF` [document loader](https://python.langchain.com/docs/concepts/document_loaders). For detailed documentation of all DocumentLoader features and configurations head to the [API reference](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.pdf.ZeroxPDFLoader.html).\n",
    "\n",
    "## Overview\n",
    "`ZeroxPDFLoader` is a document loader that leverages the [Zerox](https://github.com/getomni-ai/zerox) library. Zerox converts PDF documents into images, processes them using a vision-capable language model, and generates a structured Markdown representation. This loader allows for asynchronous operations and provides page-level document extraction.\n",
    "\n",
    "### Integration details\n",
    "\n",
    "| Class                                                                                                                                             | Package | Local | Serializable | JS support|\n",
    "|:--------------------------------------------------------------------------------------------------------------------------------------------------| :--- | :---: | :---: |  :---: |\n",
    "| [ZeroxPDFLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.pdf.ZeroxPDFLoader.html) | [langchain_community](https://python.langchain.com/api_reference/community/index.html) | ❌ | ❌ | ❌ |\n",
    "   \n",
    "---------   \n",
    "\n",
    "### Loader features\n",
    "\n",
    "|   Source    | Document Lazy Loading | Native Async Support | Extract Images | Extract Tables |\n",
    "|:-----------:| :---: | :---: | :---: |:---: |\n",
    "| ZeroxPDFLoader | ✅ | ❌ | ✅ | ✅  |\n",
    "\n",
    "  \n",
    "\n",
    "## Setup\n",
    "\n",
    "### Credentials\n",
    "Appropriate credentials need to be set up in environment variables. The loader supports number of different models and model providers. See _Usage_ header below to see few examples or [Zerox documentation](https://github.com/getomni-ai/zerox) for a full list of supported models.\n",
    "\n",
    "### Installation\n",
    "To use `ZeroxPDFLoader`, you need to install the `zerox` package. Also make sure to have `langchain-community` installed.\n",
    "\n",
    "```bash\n",
    "pip install zerox langchain-community\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "\n",
    "`ZeroxPDFLoader` enables PDF text extraction using vision-capable language models by converting each page into an image and processing it asynchronously. To use this loader, you need to specify a model and configure any necessary environment variables for Zerox, such as API keys.\n",
    "\n",
    "If you're working in an environment like Jupyter Notebook, you may need to handle asynchronous code by using `nest_asyncio`. You can set this up as follows:\n",
    "\n",
    "```python\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T07:44:38.034506Z",
     "start_time": "2024-12-13T07:44:34.992703Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "# use nest_asyncio (only necessary inside of jupyter notebook)\n",
    "import nest_asyncio\n",
    "from patch_langchain_community.document_loaders.pdf import ZeroxPDFLoader\n",
    "\n",
    "nest_asyncio.apply()\n",
    "load_dotenv()\n",
    "from getpass import getpass\n",
    "\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"OpenAI API key =\")\n",
    "file_path = \"./example_data/layout-parser-paper.pdf\"\n",
    "loader = ZeroxPDFLoader(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T07:45:25.754627Z",
     "start_time": "2024-12-13T07:44:40.500187Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "images_to_text can not be simulate\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001B[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001B[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n"
     ]
    },
    {
     "ename": "ModelAccessError",
     "evalue": "\n    Your provided model can't be accessed. Please make sure you have access to the model and also required environment variables are setup correctly including valid api key(s).\n    Refer: https://docs.litellm.ai/docs/providers\n     (Extra Info: {'model': 'gpt-4o-mini'})",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModelAccessError\u001B[0m                          Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m docs \u001B[38;5;241m=\u001B[39m \u001B[43mloader\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      2\u001B[0m docs[\u001B[38;5;241m0\u001B[39m]\n",
      "File \u001B[0;32m~/workspace.bda/patch_langchain_common/.venv/lib/python3.12/site-packages/langchain_core/document_loaders/base.py:31\u001B[0m, in \u001B[0;36mBaseLoader.load\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     29\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mload\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mlist\u001B[39m[Document]:\n\u001B[1;32m     30\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Load data into Document objects.\"\"\"\u001B[39;00m\n\u001B[0;32m---> 31\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlazy_load\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/workspace.bda/patch_langchain_common/.venv/lib/python3.12/site-packages/patch_langchain_community/document_loaders/pdf.py:1551\u001B[0m, in \u001B[0;36mZeroxPDFLoader.lazy_load\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1549\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1550\u001B[0m     blob \u001B[38;5;241m=\u001B[39m Blob\u001B[38;5;241m.\u001B[39mfrom_path(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfile_path)  \u001B[38;5;66;03m# type: ignore[attr-defined]\u001B[39;00m\n\u001B[0;32m-> 1551\u001B[0m \u001B[38;5;28;01myield from\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mparser\u001B[38;5;241m.\u001B[39mlazy_parse(blob)\n",
      "File \u001B[0;32m~/workspace.bda/patch_langchain_common/.venv/lib/python3.12/site-packages/patch_langchain_community/document_loaders/parsers/pdf.py:2168\u001B[0m, in \u001B[0;36mZeroxPDFParser.lazy_parse\u001B[0;34m(self, blob)\u001B[0m\n\u001B[1;32m   2164\u001B[0m         prompt_images \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   2165\u001B[0m     zerox_prompt \u001B[38;5;241m=\u001B[39m ZeroxPDFParser\u001B[38;5;241m.\u001B[39m_prompt\u001B[38;5;241m.\u001B[39mformat(\n\u001B[1;32m   2166\u001B[0m         prompt_tables\u001B[38;5;241m=\u001B[39mprompt_tables, prompt_images\u001B[38;5;241m=\u001B[39mprompt_images\n\u001B[1;32m   2167\u001B[0m     )\n\u001B[0;32m-> 2168\u001B[0m zerox_output \u001B[38;5;241m=\u001B[39m \u001B[43masyncio\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2169\u001B[0m \u001B[43m    \u001B[49m\u001B[43mzerox\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2170\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfile_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mfile_path\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2171\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2172\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcleanup\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcleanup\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2173\u001B[0m \u001B[43m        \u001B[49m\u001B[43mconcurrency\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconcurrency\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2174\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmaintain_format\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmaintain_format\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2175\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcustom_system_prompt\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mzerox_prompt\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2176\u001B[0m \u001B[43m        \u001B[49m\u001B[43mselect_pages\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mselect_pages\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2177\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mzerox_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2178\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2179\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2181\u001B[0m \u001B[38;5;66;03m# Convert zerox output to Document instances and yield them\u001B[39;00m\n\u001B[1;32m   2182\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(zerox_output\u001B[38;5;241m.\u001B[39mpages) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "File \u001B[0;32m~/workspace.bda/patch_langchain_common/.venv/lib/python3.12/site-packages/nest_asyncio.py:30\u001B[0m, in \u001B[0;36m_patch_asyncio.<locals>.run\u001B[0;34m(main, debug)\u001B[0m\n\u001B[1;32m     28\u001B[0m task \u001B[38;5;241m=\u001B[39m asyncio\u001B[38;5;241m.\u001B[39mensure_future(main)\n\u001B[1;32m     29\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 30\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mloop\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_until_complete\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtask\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     31\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m     32\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m task\u001B[38;5;241m.\u001B[39mdone():\n",
      "File \u001B[0;32m~/workspace.bda/patch_langchain_common/.venv/lib/python3.12/site-packages/nest_asyncio.py:98\u001B[0m, in \u001B[0;36m_patch_loop.<locals>.run_until_complete\u001B[0;34m(self, future)\u001B[0m\n\u001B[1;32m     95\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m f\u001B[38;5;241m.\u001B[39mdone():\n\u001B[1;32m     96\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m     97\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mEvent loop stopped before Future completed.\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m---> 98\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mresult\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.12/asyncio/futures.py:203\u001B[0m, in \u001B[0;36mFuture.result\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    201\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__log_traceback \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m    202\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_exception \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 203\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_exception\u001B[38;5;241m.\u001B[39mwith_traceback(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_exception_tb)\n\u001B[1;32m    204\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_result\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.12/asyncio/tasks.py:314\u001B[0m, in \u001B[0;36mTask.__step_run_and_handle_result\u001B[0;34m(***failed resolving arguments***)\u001B[0m\n\u001B[1;32m    310\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    311\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m exc \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    312\u001B[0m         \u001B[38;5;66;03m# We use the `send` method directly, because coroutines\u001B[39;00m\n\u001B[1;32m    313\u001B[0m         \u001B[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001B[39;00m\n\u001B[0;32m--> 314\u001B[0m         result \u001B[38;5;241m=\u001B[39m \u001B[43mcoro\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m    315\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    316\u001B[0m         result \u001B[38;5;241m=\u001B[39m coro\u001B[38;5;241m.\u001B[39mthrow(exc)\n",
      "File \u001B[0;32m~/workspace.bda/patch_langchain_common/.venv/lib/python3.12/site-packages/pyzerox/core/zerox.py:76\u001B[0m, in \u001B[0;36mzerox\u001B[0;34m(cleanup, concurrency, file_path, maintain_format, model, output_dir, temp_dir, custom_system_prompt, select_pages, **kwargs)\u001B[0m\n\u001B[1;32m     73\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m FileUnavailable()\n\u001B[1;32m     75\u001B[0m \u001B[38;5;66;03m# Create an instance of the litellm model interface\u001B[39;00m\n\u001B[0;32m---> 76\u001B[0m vision_model \u001B[38;5;241m=\u001B[39m \u001B[43mlitellmmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     78\u001B[0m \u001B[38;5;66;03m# override the system prompt if a custom prompt is provided\u001B[39;00m\n\u001B[1;32m     79\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m custom_system_prompt:\n",
      "File \u001B[0;32m~/workspace.bda/patch_langchain_common/.venv/lib/python3.12/site-packages/pyzerox/models/modellitellm.py:39\u001B[0m, in \u001B[0;36mlitellmmodel.__init__\u001B[0;34m(self, model, **kwargs)\u001B[0m\n\u001B[1;32m     37\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvalidate_environment()\n\u001B[1;32m     38\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvalidate_model()\n\u001B[0;32m---> 39\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvalidate_access\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/workspace.bda/patch_langchain_common/.venv/lib/python3.12/site-packages/pyzerox/models/modellitellm.py:71\u001B[0m, in \u001B[0;36mlitellmmodel.validate_access\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     69\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Validates access to the model -> if environment variables are set correctly with correct values.\"\"\"\u001B[39;00m\n\u001B[1;32m     70\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m litellm\u001B[38;5;241m.\u001B[39mcheck_valid_key(model\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel,api_key\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m---> 71\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m ModelAccessError(extra_info\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel})\n",
      "\u001B[0;31mModelAccessError\u001B[0m: \n    Your provided model can't be accessed. Please make sure you have access to the model and also required environment variables are setup correctly including valid api key(s).\n    Refer: https://docs.litellm.ai/docs/providers\n     (Extra Info: {'model': 'gpt-4o-mini'})"
     ]
    }
   ],
   "source": [
    "docs = loader.load()\n",
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T07:46:10.287363Z",
     "start_time": "2024-12-13T07:46:10.284371Z"
    }
   },
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "pprint.pp(docs[0].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lazy Load\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T07:46:59.781582Z",
     "start_time": "2024-12-13T07:46:17.160126Z"
    }
   },
   "outputs": [],
   "source": [
    "pages = []\n",
    "for doc in loader.lazy_load():\n",
    "    pages.append(doc)\n",
    "    if len(pages) >= 10:\n",
    "        # do some paged operation, e.g.\n",
    "        # index.upsert(page)\n",
    "\n",
    "        pages = []\n",
    "len(pages)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(pages[0].page_content[:100])\n",
    "pprint.pp(pages[0].metadata)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The metadata attribute contains at least the following keys:\n",
    "- source\n",
    "- page (if in mode *page*)\n",
    "- total_page\n",
    "- creationdate\n",
    "- creator\n",
    "- producer\n",
    "\n",
    "Additional metadata are specific to each parser.\n",
    "These pieces of information can be helpful (to categorize your PDFs for example)."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Splitting mode & custom pages delimiter"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "When loading the PDF file you can split it in two different ways:\n",
    "- By page\n",
    "- As a single text flow\n",
    "\n",
    "By default ZeroxPDFLoader will split the PDF by page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Extract the PDF by page. Each page is extracted as a langchain Document object:"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "loader = ZeroxPDFLoader(\n",
    "    \"./example_data/layout-parser-paper.pdf\",\n",
    "    mode=\"page\",\n",
    ")\n",
    "docs = loader.load()\n",
    "print(len(docs))\n",
    "pprint.pp(docs[0].metadata)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "In this mode the pdf is split by pages and the resulting Documents metadata contains the page number. But in some cases we could want to process the pdf as a single text flow (so we don't cut some paragraphs in half). In this case you can use the *single* mode :"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Extract the whole PDF as a single langchain Document object:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T15:13:05.016680Z",
     "start_time": "2024-12-11T15:13:04.739002Z"
    }
   },
   "outputs": [],
   "source": [
    "loader = ZeroxPDFLoader(\n",
    "    \"./example_data/layout-parser-paper.pdf\",\n",
    "    mode=\"single\",\n",
    ")\n",
    "docs = loader.load()\n",
    "print(len(docs))\n",
    "pprint.pp(docs[0].metadata)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Logically, in this mode, the ‘page_number’ metadata disappears. Here's how to clearly identify where pages end in the text flow :"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Add a custom *pages_delimitor* to identify where are ends of pages in *single* mode:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T15:13:05.412714Z",
     "start_time": "2024-12-11T15:13:05.034893Z"
    }
   },
   "outputs": [],
   "source": [
    "loader = ZeroxPDFLoader(\n",
    "    \"./example_data/layout-parser-paper.pdf\",\n",
    "    mode=\"single\",\n",
    "    pages_delimitor=\"\\n-------THIS IS A CUSTOM END OF PAGE-------\\n\",\n",
    ")\n",
    "docs = loader.load()\n",
    "print(docs[0].page_content[:5780])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "This could simply be \\n, or \\f to clearly indicate a page change, or \\<!-- PAGE BREAK --> for seamless injection in a Markdown viewer without a visual effect."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Extract images from the PDF"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "ZeroxPDFLoader is able to extract images from your PDFs."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T15:13:41.254995Z",
     "start_time": "2024-12-11T15:13:06.693829Z"
    }
   },
   "outputs": [],
   "source": [
    "from patch_langchain_community.document_loaders.parsers.pdf import (\n",
    "    convert_images_to_description,\n",
    ")\n",
    "\n",
    "loader = ZeroxPDFLoader(\n",
    "    \"./example_data/layout-parser-paper.pdf\",\n",
    "    mode=\"page\",\n",
    "    extract_images=True,\n",
    "    images_to_text=convert_images_to_description(model=None, format=\"html\"),\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "print(docs[5].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API reference\n",
    "\n",
    "### `ZeroxPDFLoader`\n",
    "\n",
    "This loader class initializes with a file path and model type, and supports custom configurations via `zerox_kwargs` for handling Zerox-specific parameters.\n",
    "\n",
    "**Arguments**:\n",
    "- `file_path` (Union[str, Path]): Path to the PDF file.\n",
    "- `model` (str): Vision-capable model to use for processing in format `<provider>/<model>`.\n",
    "Some examples of valid values are: \n",
    "  - `model = \"gpt-4o-mini\" ## openai model`\n",
    "  - `model = \"azure/gpt-4o-mini\"`\n",
    "  - `model = \"gemini/gpt-4o-mini\"`\n",
    "  - `model=\"claude-3-opus-20240229\"`\n",
    "  - `model = \"vertex_ai/gemini-1.5-flash-001\"`\n",
    "  - See more details in [Zerox documentation](https://github.com/getomni-ai/zerox)\n",
    "  - Defaults to `\"gpt-4o-mini\".`\n",
    "- `**zerox_kwargs` (dict): Additional Zerox-specific parameters such as API key, endpoint, etc.\n",
    "  - See [Zerox documentation](https://github.com/getomni-ai/zerox)\n",
    "\n",
    "**Methods**:\n",
    "- `lazy_load`: Generates an iterator of `Document` instances, each representing a page of the PDF, along with metadata including page number and source.\n",
    "\n",
    "See full API documentaton [here](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.pdf.ZeroxPDFLoader.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "- **Model Compatibility**: Zerox supports a range of vision-capable models. Refer to [Zerox's GitHub documentation](https://github.com/getomni-ai/zerox) for a list of supported models and configuration details.\n",
    "- **Environment Variables**: Make sure to set required environment variables, such as `API_KEY` or endpoint details, as specified in the Zerox documentation.\n",
    "- **Asynchronous Processing**: If you encounter errors related to event loops in Jupyter Notebooks, you may need to apply `nest_asyncio` as shown in the setup section.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Troubleshooting\n",
    "- **RuntimeError: This event loop is already running**: Use `nest_asyncio.apply()` to prevent asynchronous loop conflicts in environments like Jupyter.\n",
    "- **Configuration Errors**: Verify that the `zerox_kwargs` match the expected arguments for your chosen model and that all necessary environment variables are set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "- **Zerox Documentation**: [Zerox GitHub Repository](https://github.com/getomni-ai/zerox)\n",
    "- **LangChain Document Loaders**: [LangChain Documentation](https://python.langchain.com/docs/integrations/document_loaders/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "patch-langchain",
   "language": "python",
   "name": "patch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
